{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtkTPRvE8JrR"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import PorterStemmer\n",
        "# import re\n",
        "\n",
        "# # Download the NLTK data\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# # Load the dataset from CSV file\n",
        "# df = pd.read_csv('/path/to/imdb/dataset.csv')\n",
        "\n",
        "# # Remove any rows with missing values\n",
        "# df.dropna(inplace=True)\n",
        "\n",
        "# # Convert the sentiment column to binary labels (0 for negative, 1 for positive)\n",
        "# df['sentiment'] = np.where(df['sentiment'] == 'positive', 1, 0)\n",
        "\n",
        "# # Define a function to preprocess the text\n",
        "# def preprocess_text(text):\n",
        "#     # Convert all text to lowercase\n",
        "#     text = text.lower()\n",
        "\n",
        "#     # Remove any HTML tags from the text\n",
        "#     text = re.sub('<[^>]+>', '', text)\n",
        "\n",
        "#     # Tokenize the text into individual words\n",
        "#     tokens = nltk.word_tokenize(text)\n",
        "\n",
        "#     # Remove any stop words from the tokens\n",
        "#     stop_words = set(stopwords.words('english'))\n",
        "#     filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "#     # Stem the tokens using the Porter stemmer\n",
        "#     stemmer = PorterStemmer()\n",
        "#     stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "\n",
        "#     # Join the stemmed tokens back into a single string\n",
        "#     processed_text = ' '.join(stemmed_tokens)\n",
        "\n",
        "#     return processed_text\n",
        "\n",
        "# # Apply the preprocessing function to the 'reviews' column of the dataframe\n",
        "# df['reviews'] = df['reviews'].apply(preprocess_text)\n",
        "\n",
        "# # Save the preprocessed dataframe to a new CSV file\n",
        "# df.to_csv('/path/to/preprocessed/imdb/dataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pndrbEjW8JrV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVNuguGE8JrW",
        "outputId": "ee5ef8a6-b76f-45c7-ed3d-eae64759bc57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\JOHN\n",
            "[nltk_data]     WICK\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Download the NLTK data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpunkt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mE:\\Python\\lib\\site-packages\\nltk\\downloader.py:779\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(s, prefix2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    771\u001b[0m     print_to(\n\u001b[0;32m    772\u001b[0m         textwrap\u001b[38;5;241m.\u001b[39mfill(\n\u001b[0;32m    773\u001b[0m             s,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    776\u001b[0m         )\n\u001b[0;32m    777\u001b[0m     )\n\u001b[1;32m--> 779\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincr_download(info_or_id, download_dir, force):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# Error messages\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(msg, ErrorMessage):\n\u001b[0;32m    782\u001b[0m         show(msg\u001b[38;5;241m.\u001b[39mmessage)\n",
            "File \u001b[1;32mE:\\Python\\lib\\site-packages\\nltk\\downloader.py:643\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m FinishCollectionMessage(info)\n\u001b[0;32m    641\u001b[0m \u001b[38;5;66;03m# Handle Packages (delegate to a helper function).\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_package(info, download_dir, force):\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m msg\n",
            "File \u001b[1;32mE:\\Python\\lib\\site-packages\\nltk\\downloader.py:682\u001b[0m, in \u001b[0;36mDownloader._download_package\u001b[1;34m(self, info, download_dir, force)\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m ProgressMessage(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# Do we already have the current version?\u001b[39;00m\n\u001b[1;32m--> 682\u001b[0m status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force \u001b[38;5;129;01mand\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mINSTALLED:\n\u001b[0;32m    684\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m UpToDateMessage(info)\n",
            "File \u001b[1;32mE:\\Python\\lib\\site-packages\\nltk\\downloader.py:885\u001b[0m, in \u001b[0;36mDownloader.status\u001b[1;34m(self, info_or_id, download_dir)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_status_cache:\n\u001b[1;32m--> 885\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_status_cache[info\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pkg_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_status_cache[info\u001b[38;5;241m.\u001b[39mid]\n",
            "File \u001b[1;32mE:\\Python\\lib\\site-packages\\nltk\\downloader.py:901\u001b[0m, in \u001b[0;36mDownloader._pkg_status\u001b[1;34m(self, info, filepath)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTALE\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# Check if the file's checksum matches\u001b[39;00m\n\u001b[1;32m--> 901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmd5_hexdigest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m info\u001b[38;5;241m.\u001b[39mchecksum:\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTALE\n\u001b[0;32m    904\u001b[0m \u001b[38;5;66;03m# If it's a zipfile, and it's been at least partially\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# unzipped, then check if it's been fully unzipped.\u001b[39;00m\n",
            "File \u001b[1;32mE:\\Python\\lib\\site-packages\\nltk\\downloader.py:2210\u001b[0m, in \u001b[0;36mmd5_hexdigest\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m   2205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m \u001b[38;5;124;03mCalculate and return the MD5 checksum for a given file.\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m \u001b[38;5;124;03m``file`` may either be a filename or an open stream.\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 2210\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[0;32m   2211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _md5_hexdigest(infile)\n\u001b[0;32m   2212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _md5_hexdigest(file)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Download the NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_KFQuaQ8JrY"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from CSV file\n",
        "df = pd.read_csv('IMDB Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-xeFT2G8JrZ"
      },
      "outputs": [],
      "source": [
        "# Remove any rows with missing values\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T42j2J_8Jra"
      },
      "outputs": [],
      "source": [
        "# Convert the sentiment column to binary labels (0 for negative, 1 for positive)\n",
        "#df['sentiment'] = np.where(df['sentiment'] == 'positive', 1, 0) #probably error ais se araha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSBwbGJL8Jra"
      },
      "outputs": [],
      "source": [
        "# Define a function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Convert all text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove any HTML tags from the text\n",
        "    text = re.sub('<[^>]+>', '', text)\n",
        "\n",
        "    # Tokenize the text into individual words //\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove any stop words from the tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    # Stem the tokens using the Porter stemmer //\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "\n",
        "    # Join the stemmed tokens back into a single string\n",
        "    processed_text = ' '.join(stemmed_tokens)\n",
        "\n",
        "    return processed_text\n",
        "    #return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWaORjcg8Jra"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Define function to clean text\n",
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize text into words\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    # Apply lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    # Handle negations\n",
        "    words = [word if word != \"not\" else \"not_\" for word in words]\n",
        "    # Remove noisy data\n",
        "    words = [word for word in words if len(word) > 1]\n",
        "    # Join words back into text\n",
        "    text = \" \".join(words)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkWCnhDt8Jrb"
      },
      "outputs": [],
      "source": [
        "# Apply the preprocessing function to the 'review' column of the dataframe\n",
        "#df['review'] = df['review'].apply(preprocess_text)\n",
        "df['review'] = df['review'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1816IrZy8Jrc"
      },
      "outputs": [],
      "source": [
        "# Save the preprocessed dataframe to a new CSV file\n",
        "df.to_csv('IMDB Dataset Preprocessed.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6S5vNsC8Jrc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}